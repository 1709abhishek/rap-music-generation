# -*- coding: utf-8 -*-
"""Lyrics Gen Model (distilgpt2-rap).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v08TZV65OAxifD7OSmNYDSBIj2kFWGQX
"""

!pip install transformers
!pip install bitsandbytes
!pip install datasets
!pip install accelerate

from google.colab import drive
drive.mount('/content/MyDrive')

import transformers
import torch
import torch.nn.functional as F
from torch import nn
from torch.cuda.amp import custom_fwd, custom_bwd
from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise
from tqdm.auto import tqdm

#config = transformers.GPTJConfig.from_pretrained(f"/content/drive/MyDrive/ML Bootcamp/Capstone/lyric_generation/gpt-j-6b/checkpoint-{checkpoint_num}")
config = transformers.GPTJConfig.from_pretrained("dzionek/distilgpt2-rap")
tokenizer = transformers.AutoTokenizer.from_pretrained("dzionek/distilgpt2-rap")

import random
from datasets import Dataset, DatasetDict

def randomize_lines(data_path):
    with open(data_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
        print(len(lines))
        random.shuffle(lines)
        return lines

train_data_path = '/content/MyDrive/MyDrive/NLP Project/train_couplets.txt'
test_data_path = '/content/MyDrive/MyDrive/NLP Project/test_couplets.txt'

train_lines = randomize_lines(train_data_path)
test_lines = randomize_lines(test_data_path)

train_dataset = Dataset.from_dict({"text": train_lines})
test_dataset = Dataset.from_dict({"text": test_lines})


# Concatenate train and test datasets into a single dataset
dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

tokenizer = transformers.AutoTokenizer.from_pretrained("dzionek/distilgpt2-rap")

def tokenize_function(examples):
  return tokenizer(examples["text"])

tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns="text")

del tokenizer
del dataset

block_size = 128
checkpoint_num = 8000

def group_texts(examples):
  # Concatenate all texts.
  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
  #print(concatenated_examples)
  total_length = len(concatenated_examples[list(examples.keys())[0]])
  #print(f"Total length: {total_length}")
  total_length = (total_length // block_size) * block_size
  # Split by chunks of max_len.
  result = {
    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
    for k, t in concatenated_examples.items()
  }
  result["labels"] = result["input_ids"].copy()
  return result

verses_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=500,
    num_proc=8,
)

from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("dzionek/distilgpt2-rap")

from transformers import Trainer, TrainingArguments, AutoModelForCausalLM

training_args = TrainingArguments(
    output_dir="/content", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    num_train_epochs=100, # number of training epochs
    # evaluation_strategy="steps",
    # per_device_train_batch_size=4, # batch size for training
    # per_device_eval_batch_size=4,  # batch size for evaluation
    # # eval_steps = 50, # Number of update steps between two evaluations.
    # # save_steps = 2000, # after # steps model is saved
    # save_total_limit = 999,# limits total # of checkpoints
    # load_best_model_at_end = True, # save best checkpoint after training complete
    # warmup_steps = 50,# number of warmup steps for learning rate scheduler
    # # prediction_loss_only=True,
    # save_strategy="steps",  #changed from "steps" to "no" to try to use callback to save entire model
    # weight_decay=0.001,
    # resume_from_checkpoint=f"/content-{checkpoint_num}")
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=verses_datasets['train'],
    eval_dataset=verses_datasets['test'],
    )
    #callbacks=[SaveCallback])

trainer.train()
# print(trainer)

import re
def generate_verse():
    prompt = "And that ass feel like jello (jello)"
    device = torch.cuda.current_device()

    print("...getting prediction...")
    tokenizer = transformers.AutoTokenizer.from_pretrained("dzionek/distilgpt2-rap")
    # model = model
    with torch.no_grad():
        result_length = 75
        prompt = "~ " + prompt + " =1G->2G= "
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        beam_outputs = model.generate(inputs["input_ids"],
            max_length=result_length,
            top_k=50, top_p=0.95,
            do_sample=True, temperature=0.7, pad_token_id=50256,
            num_return_sequences=10)

        lines = []
        for beam in beam_outputs:
            text = tokenizer.decode(beam, skip_special_tokens=True)
            line = text.split(" =1G->2G= ")[1]
            line = line[:line.find(" ~")]
            if line not in lines:
                lines.append(line.strip("'").strip('"'))

        filtered_lines = [replace_n_word(line) for line in lines] #remove n-word
        # filtered_lines = [line for line in filtered_lines if has_characters(line)]

        if len(filtered_lines) == 0:
            return generate_verse()  # Recursively retry if no suitable lines found

        # Delete unnecessary variables to conserve memory
        del inputs, beam_outputs, lines

        return filtered_lines

def replace_n_word(line):
    # Define a regular expression pattern to match n-word and hyphenated cases
    pattern = r'\b\w*-?nigga\w*\b'
    # Use re.search to find the pattern in the line
    line = re.sub(pattern,"*****", line, flags=re.IGNORECASE)
    # returns line with stars if present or w no modifications if word not found
    return line

def has_characters(self,line):
# Strip removes leading and trailing whitespaces including tabs and newlines
     return False if line.strip() == "" else True

x = generate_verse()

print(x)

# remove whatever is there between equals

# unphonemize and reverse engineer the generated verses

tokenizer = transformers.AutoTokenizer.from_pretrained("dzionek/distilgpt2-rap")
input_ids = tokenizer.encode("take it easy start with a bat", return_tensors='pt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Move the input_ids tensor to the same device as the model
input_ids = input_ids.to(device)
output = model.generate(input_ids, max_length=100, num_return_sequences=5, do_sample=True, top_p=0.9)
print(tokenizer.decode(output[0], skip_special_tokens=True))

tokenizer = transformers.AutoTokenizer.from_pretrained("dzionek/distilgpt2-rap")
input_ids = tokenizer.encode("from concrete jungles to stars above, this beats the canvas, lets paint with blood", return_tensors='pt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Move the input_ids tensor to the same device as the model
input_ids = input_ids.to(device)
output = model.generate(input_ids, max_length=100, num_return_sequences=5, do_sample=True, top_p=0.9)
print(tokenizer.decode(output[0], skip_special_tokens=True))

tokenizer = transformers.AutoTokenizer.from_pretrained("dzionek/distilgpt2-rap")
input_ids = tokenizer.encode("Mom's spaghetti knees weak arms are heavy", return_tensors='pt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Move the input_ids tensor to the same device as the model
input_ids = input_ids.to(device)
output = model.generate(input_ids, max_length=100, num_return_sequences=5, do_sample=True, top_p=0.9)
print(tokenizer.decode(output[0], skip_special_tokens=True))

!pip install eng_to_ipa
import eng_to_ipa as ipa

def is_vowel(character):

    ipa_vowels = "aeiou"
    return character in ipa_vowels

def is_whitespace(character):
    return character==' ' or character=='\n'

def get_phonetic_transcription(lyrics):
    lines = lyrics.splitlines()
    phonetic_lines = [ipa.convert(line) for line in lines]
    phonetic = "\n".join(phonetic_lines)

    return phonetic

import re
import numpy as np

def syllable_similarity(bar1, bar2):
    similarity = abs(syllables.estimate(bar1) - syllables.estimate(bar2))
    return similarity

def get_syllable_count_difference(lyrics):
    lines = lyrics.splitlines()
    i = 0
    similarity = 0
    while i < len(lines)-1:
        similarity += syllable_similarity(lines[i], lines[i+1])
        i += 2
    return similarity/len(lines)

def get_rhyme_density(lyrics, lookback=15):
    bars = SongLyrics(lyrics, lookback)
    return bars.avg_rhyme_length

def get_longest_rhyme(lyrics, lookback=15):
    bars = SongLyrics(lyrics, lookback)
    return bars.get_longest_rhyme()[0]

def get_unique_words(lyrics):
    words = lyrics.split()
    unique_words = set(words)
    return len(unique_words)/len(words)

def print_lyrics_stats(lyrics, lookback=15, artist=None, title=None):
    bars = SongLyrics(lyrics, lookback, artist, title)
    print(bars.title)
    print('------------------------------------------')
    print("Average rhyme length: %.3f\n" % bars.avg_rhyme_length)
    bars.print_rhyme(bars.longest_rhyme)
    print("Average syllable count difference: %.3f\n" % get_syllable_count_difference(lyrics))
    print("Percentage unique words: %.3f\n" % get_unique_words(lyrics))



class SongLyrics:
    '''
    This class is used to store and preprocess rap lyrics and calculate
    statistics like average rhyme length out of the lyrics.
    '''

    def __init__(self, text=None, lookback=15, artist=None, title=None):
        '''
        Lyrics can be read from the file (default) or passed directly
        to this constructor.
        '''
        self.raw_text = None
        # How many previous words are checked for a rhyme.
        self.lookback = lookback
        self.raw_text = text
        if artist == None or title == None:
            self.title = "Generated Song"
        else:
            self.title = title + " by " + artist

        if self.raw_text is not None:
            cleaning_ok = self.clean_text(self.raw_text)
            self.compute_vowel_representation()
            self.avg_rhyme_length, self.longest_rhyme = self.calculate_rhyme_stats()

    def clean_text(self, text):
        '''
        Preprocess text by removing unwanted characters and duplicate rows.
        '''

        self.text = text
        # If there are more than 2 consecutive newlines, remove some of them
        # (just to make the cleaned text look prettier)
        self.text = re.sub('\n\n+', '\n\n', self.text)
        # Remove duplicate rows
        self.lines = self.text.split('\n')

        unique_lines = set()
        new_text = ''
        for line in self.lines:
            line = line.strip()
            if len(line) > 0 and line in unique_lines:
                continue
            # Remove lines that are within brackets/parenthesis
            if len(line) >= 2 and ((line[0]=='[' and line[-1]==']') or (line[0]=='(' and line[-1]==')')):
                continue
            unique_lines.add(line)
            new_text += line + '\n'

        self.text = new_text

    def compute_vowel_representation(self):
        '''
        Compute a representation of the lyrics where only vowels are preserved.
        '''
        self.vowels_only = [] # Lyrics with all but vowels removed
        self.vowel_indices = [] # Indices of the vowels in self.text list
        self.word_end_indices = [] # Indices of the last characters of each word
        self.words = [] # List of words in the lyrics
        self.line_indices = []

        self.original_text = self.text
        self.text = get_phonetic_transcription(self.text)
        self.original_word_end_indices = []
        self.original_words = []

        previous_space_index = -1 # Index of the previous space char
        line_index = 0 # Line index of the current character
        # Go through the lyrics char by char
        for i in range(len(self.text)):
            self.line_indices.append(line_index)
            character = self.text[i]
            if is_vowel(character):
                # Ignore double vowels
                if i > 0 and self.text[i-1] == character:
                    # Index of a double vowel points to the latter occurrence
                    self.vowel_indices[-1] = i
                    continue
                self.vowels_only.append(character)
                self.vowel_indices.append(i)
            elif is_whitespace(character):
                if character in '\n':
                    line_index += 1
                elif character in '.!?' and i < len(self.text)-1 and self.text[i+1] != '\n':
                    line_index += 1
                if len(self.vowels_only) > 0 and not is_whitespace(self.text[i-1]):
                    new_word = self.text[previous_space_index+1:self.vowel_indices[-1]+1]
                    no_vowels = True
                    for char in new_word:
                        if is_vowel(char):
                            no_vowels = False
                            break
                    if no_vowels:
                        previous_space_index = i
                        continue
                    self.word_end_indices.append(len(self.vowels_only)-1)
                    self.words.append(new_word)
                previous_space_index = i

        self.original_lines = self.original_text.split('\n')

    def rhyme_length(self, word_position_2):
        '''
        Length of rhyme (in vowels). The latter part of the rhyme ends with
        word self.words[word_position_2].

        Input:
            word_position_2       Word index of the end of the rhyme.
        '''
        max_length = 0
        max_word_position_1 = None
        word_position_1 = max(0, word_position_2 - self.lookback)
        while word_position_1 < word_position_2:
            rhyme_length = self.fixed_rhyme_length(word_position_1, word_position_2)
            if rhyme_length > max_length:
                max_length = rhyme_length
                max_word_position_1 = word_position_1
            word_position_1 += 1
        return max_length, max_word_position_1

    def fixed_rhyme_length(self, word_position_1, word_position_2):
        '''
        Length of rhyme (in vowels). The first part of the rhyme ends with
        self.words[word_position_1] and the latter part with word self.words[word_position_2].

        Input:
            word_position_1       Word index of the last word in the first part of the rhyme.
            word_position_2       Word index of the end of the rhyme.
        '''
        if word_position_1 < 0:
            return 0
        elif self.words[word_position_1] == self.words[word_position_2]:
            return 0
        vowel_index_1 = self.word_end_indices[word_position_1]
        vowel_index_2 = self.word_end_indices[word_position_2]
        length = 0
        while self.vowels_only[vowel_index_1 - length] == self.vowels_only[vowel_index_2 - length]:
            if word_position_1 > 0 and vowel_index_1 - length <= self.word_end_indices[word_position_1 - 1] and word_position_2 > 0 and vowel_index_2 - length <= self.word_end_indices[word_position_2 - 1]:
                prev_start_index_1 = self.vowel_indices[vowel_index_1 - length]
                while prev_start_index_1 > 0 and not is_whitespace(self.text[prev_start_index_1 - 1]):
                    prev_start_index_1 -= 1
                prev_start_index_2 = self.vowel_indices[vowel_index_2 - length]
                while prev_start_index_2 > 0 and not is_whitespace(self.text[prev_start_index_2 - 1]):
                    prev_start_index_2 -= 1
                next_start_index_1 = self.vowel_indices[vowel_index_1 - length]
                while next_start_index_1 < len(self.text) - 1 and not is_whitespace(self.text[next_start_index_1 + 1]):
                    next_start_index_1 += 1
                next_start_index_2 = self.vowel_indices[vowel_index_2 - length]
                while next_start_index_2 < len(self.text) - 1 and not is_whitespace(self.text[next_start_index_2 + 1]):
                    next_start_index_2 += 1
                if next_start_index_1 - prev_start_index_1 == next_start_index_2 - prev_start_index_2 and self.text[prev_start_index_1:next_start_index_1 + 1] == self.text[prev_start_index_2:next_start_index_2 + 1]:
                    break
            length += 1
            if vowel_index_1 - length < 0 or vowel_index_2 - length <= vowel_index_1:
                break
        if length == 1:
            length = 0
        return length

    def calculate_rhyme_stats(self):
        '''
        Compute the average rhyme length of the song and the longest rhyme.

        Output:
            Average rhyme length (float)
            Longest rhyme which is a 3-tuple with:
                (length, word index of the first part of the rhyme,
                         word index of the latter part of the rhyme)
        '''
        rhyme_lengths = []
        max_rhyme = (0, None, None)
        for word_position_2 in range(1, len(self.word_end_indices)):
            rhyme_length, word_position_1 = self.rhyme_length(word_position_2)
            rhyme_lengths.append(rhyme_length)
            if rhyme_length > max_rhyme[0]:
                max_rhyme = (rhyme_length, word_position_1, word_position_2)
        rhyme_lengths = np.array(rhyme_lengths)
        if len(rhyme_lengths) > 0:
            avg_rhyme_length = np.mean(rhyme_lengths)
        else:
            avg_rhyme_length = 0
        return avg_rhyme_length, max_rhyme

    def get_avg_rhyme_length(self):
        return self.avg_rhyme_length

    def print_song_stats(self):
        print('------------------------------------------')
        print("Avg rhyme length: %.3f\n" % self.avg_rhyme_length)

        self.print_rhyme(self.longest_rhyme)

    def print_rhyme(self, rhyme_tuple):
        print(self.get_rhyme_str(rhyme_tuple))

    def get_rhyme_str(self, rhyme_tuple):
        ret = ''
        rhyme_length, word_position_1, word_position_2 = rhyme_tuple
        if word_position_1 is None or word_position_2 is None:
            return ''
        vowel_index_2 = self.vowel_indices[self.word_end_indices[word_position_2]]
        vowel_index_2_original = vowel_index_2
        while not is_whitespace(self.text[vowel_index_2]):
            vowel_index_2 += 1
        vowel_index_1 = self.vowel_indices[self.word_end_indices[word_position_1] - rhyme_length]
        vowel_index_1_original = vowel_index_1
        while self.text[vowel_index_1] != '\n' and vowel_index_1 > 0:
            vowel_index_1 -= 1

        capitalized_line = ''
        rhyming_vowels_1, rhyming_vowels_2 = self.get_rhyming_vowels(rhyme_tuple)
        for i in range(vowel_index_1, vowel_index_2 + 1):
            if i == min(rhyming_vowels_1) or i == min(rhyming_vowels_2):
                capitalized_line += ' | ' + self.text[i]
            elif i == max(rhyming_vowels_1) or i == max(rhyming_vowels_2):
                capitalized_line += self.text[i] + '|'
            else:
                capitalized_line += self.text[i]
        ret += "Longest rhyme (l=%d): %s\n" % (rhyme_length, capitalized_line)
        line_begin = self.line_indices[vowel_index_1]
        line_end = self.line_indices[vowel_index_2]
        for i in range(line_begin, line_end + 1):
            if i < len(self.original_lines):
                ret += self.original_lines[i] + '\n'
        return ret

    def get_longest_rhyme(self):
        rhyme_str = self.get_rhyme_str(self.longest_rhyme)
        return self.longest_rhyme[0], rhyme_str

    def get_rhyming_vowels(self, rhyme_tuple):
        rhyme_length, word_position_1, word_position_2 = rhyme_tuple
        if word_position_1 is None or word_position_2 is None:
            return ([-1], [-1])

        rhyming_vowels_1 = []
        count_caps = 0
        index = self.vowel_indices[self.word_end_indices[word_position_1]]
        while count_caps < rhyme_length:
            if is_vowel(self.text[index]):
                rhyming_vowels_1.append(index)
                if self.text[index] != self.text[index + 1]:
                    count_caps += 1
            index -= 1

        rhyming_vowels_2 = []
        count_caps = 0
        index = self.vowel_indices[self.word_end_indices[word_position_2]]
        last_index = index
        while count_caps < rhyme_length:
            if is_vowel(self.text[index]):
                rhyming_vowels_2.append(index)
                if index == last_index or self.text[index] != self.text[index + 1]:
                    count_caps += 1
            index -= 1

        return (rhyming_vowels_1, rhyming_vowels_2)

import numpy as np
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.util import bigrams, ngrams, everygrams
from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends
from nltk.lm import MLE, KneserNeyInterpolated, Lidstone, Laplace, AbsoluteDiscountingInterpolated

nltk.download('punkt')

final_verse = "Your finger, wrapped around, out iced. I'd rather keep it real with you, all over me now. Yeah. Colder getting room, but sweatin' me got drugs. High-high, get-get, getting high, getting high. A bitch can. Yeah, yeah. Yeah, yeah. You like walking and working. I know. I'm taking that to heart for you, just as I am, just the same me. Just dream. I'm dreaming, in that make me slim, me. My girl spits me in vain all day in this complex. My leather black jeans got 'em on, every time I fall you now, now."
result_final = re.sub(r'[ ]+\n[ ]+', r'\n', ''.join(final_verse))
print(result_final)

def get_rhyme_density(lyrics, lookback=15):
    bars = Lyrics(lyrics, lookback)
    return bars.avg_rhyme_length
results_rd = np.array(get_rhyme_density(result_final))

round(np.mean(results_rd),2)

results_lr = np.array(get_longest_rhyme(result_final))

round(np.mean(results_lr),2)

results_uw = np.array(get_unique_words(result_final))

print(round(np.mean(results_uw),2))