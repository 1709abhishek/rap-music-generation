# -*- coding: utf-8 -*-
"""Lyrics Gen Model (GPT2-finetuned-lyric-generation).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R2mfuqnxgjyTBehWDPZ7uENgQxPYHGY4
"""

!pip install transformers
!pip install bitsandbytes
!pip install datasets
!pip install accelerate

from google.colab import drive
drive.mount('/content/MyDrive')

import transformers
import torch
import torch.nn.functional as F
from torch import nn
from torch.cuda.amp import custom_fwd, custom_bwd
from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise
from tqdm.auto import tqdm

#config = transformers.GPTJConfig.from_pretrained(f"/content/drive/MyDrive/ML Bootcamp/Capstone/lyric_generation/gpt-j-6b/checkpoint-{checkpoint_num}")
config = transformers.GPTJConfig.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")
tokenizer = transformers.AutoTokenizer.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")

import random
from datasets import Dataset, DatasetDict

def randomize_lines(data_path):
    with open(data_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
        print(len(lines))
        random.shuffle(lines)
        return lines

train_data_path = '/content/MyDrive/MyDrive/NLP Project/train_couplets.txt'
test_data_path = '/content/MyDrive/MyDrive/NLP Project/test_couplets.txt'

train_lines = randomize_lines(train_data_path)
test_lines = randomize_lines(test_data_path)

train_dataset = Dataset.from_dict({"text": train_lines})
test_dataset = Dataset.from_dict({"text": test_lines})


# Concatenate train and test datasets into a single dataset
dataset = DatasetDict({"train": train_dataset, "test": test_dataset})

tokenizer = transformers.AutoTokenizer.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")

def tokenize_function(examples):
  return tokenizer(examples["text"])

tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns="text")

del tokenizer
del dataset

block_size = 128
checkpoint_num = 8000

def group_texts(examples):
  # Concatenate all texts.
  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
  #print(concatenated_examples)
  total_length = len(concatenated_examples[list(examples.keys())[0]])
  #print(f"Total length: {total_length}")
  total_length = (total_length // block_size) * block_size
  # Split by chunks of max_len.
  result = {
    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
    for k, t in concatenated_examples.items()
  }
  result["labels"] = result["input_ids"].copy()
  return result

verses_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=500,
    num_proc=8,
)

from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")
gpt = model

from transformers import Trainer, TrainingArguments, AutoModelForCausalLM

training_args = TrainingArguments(
    output_dir="/content/MyDrive/MyDrive/NLP Project/", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    num_train_epochs=100, # number of training epochs
    # evaluation_strategy="steps",
    per_device_train_batch_size=4, # batch size for training
    per_device_eval_batch_size=4,  # batch size for evaluation
    # # eval_steps = 50, # Number of update steps between two evaluations.
    # # save_steps = 2000, # after # steps model is saved
    # save_total_limit = 999,# limits total # of checkpoints
    # load_best_model_at_end = True, # save best checkpoint after training complete
    # warmup_steps = 50,# number of warmup steps for learning rate scheduler
    # # prediction_loss_only=True,
    # save_strategy="steps",  #changed from "steps" to "no" to try to use callback to save entire model
    weight_decay=0.001,
    # resume_from_checkpoint=f"/content-{checkpoint_num}")
)

trainer = Trainer(
    model=gpt,
    args=training_args,
    train_dataset=verses_datasets['train'],
    eval_dataset=verses_datasets['test'])
    #callbacks=[SaveCallback])

trainer.train()
# print(trainer)

torch.save(gpt, "/content/MyDrive/MyDrive/NLP Project/rap_lyrics_model.pt")

import re
def generate_verse():
    prompt = "And that ass feel like jello (jello)"
    device = torch.cuda.current_device()

    print("...getting prediction...")
    tokenizer = transformers.AutoTokenizer.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")
    model = gpt
    with torch.no_grad():
        result_length = 75
        prompt = "~ " + prompt + " =1G->2G= "
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        beam_outputs = model.generate(inputs["input_ids"],
            max_length=result_length,
            top_k=50, top_p=0.95,
            do_sample=True, temperature=0.7, pad_token_id=50256,
            num_return_sequences=10)

        lines = []
        for beam in beam_outputs:
            text = tokenizer.decode(beam, skip_special_tokens=True)
            line = text.split(" =1G->2G= ")[1]
            line = line[:line.find(" ~")]
            if line not in lines:
                lines.append(line.strip("'").strip('"'))

        filtered_lines = [replace_n_word(line) for line in lines] #remove n-word
        # filtered_lines = [line for line in filtered_lines if has_characters(line)]

        if len(filtered_lines) == 0:
            return generate_verse()  # Recursively retry if no suitable lines found

        # Delete unnecessary variables to conserve memory
        del inputs, beam_outputs, lines

        return filtered_lines

def replace_n_word(line):
    # Define a regular expression pattern to match n-word and hyphenated cases
    pattern = r'\b\w*-?nigga\w*\b'
    # Use re.search to find the pattern in the line
    line = re.sub(pattern,"*****", line, flags=re.IGNORECASE)
    # returns line with stars if present or w no modifications if word not found
    return line

def has_characters(self,line):
# Strip removes leading and trailing whitespaces including tabs and newlines
     return False if line.strip() == "" else True

x = generate_verse()

print(x)

tokenizer = transformers.AutoTokenizer.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")
input_ids = tokenizer.encode("i'm gonna hit the bars nobody ever can, i shall be honest like no other man", return_tensors='pt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Move the input_ids tensor to the same device as the model
input_ids = input_ids.to(device)
output = model.generate(input_ids, max_length=400, num_return_sequences=5, do_sample=True, top_p=0.9)
final_verse = tokenizer.decode(output[0], skip_special_tokens=True)
print(tokenizer.decode(output[0], skip_special_tokens=True))

tokenizer = transformers.AutoTokenizer.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")
input_ids = tokenizer.encode("from concrete jungles to stars above, this beats the canvas, lets paint with blood", return_tensors='pt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Move the input_ids tensor to the same device as the model
input_ids = input_ids.to(device)
output = model.generate(input_ids, max_length=400, num_return_sequences=5, do_sample=True, top_p=0.9)
final_verse = tokenizer.decode(output[0], skip_special_tokens=True)
print(tokenizer.decode(output[0], skip_special_tokens=True))

tokenizer = transformers.AutoTokenizer.from_pretrained("SpartanCinder/GPT2-finetuned-lyric-generation")
input_ids = tokenizer.encode("Mom's spaghetti knees weak arms are heavy", return_tensors='pt')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Move the input_ids tensor to the same device as the model
input_ids = input_ids.to(device)
output = model.generate(input_ids, max_length=400, num_return_sequences=5, do_sample=True, top_p=0.9)
final_verse = tokenizer.decode(output[0], skip_special_tokens=True)
print(tokenizer.decode(output[0], skip_special_tokens=True))

!pip install eng_to_ipa
import eng_to_ipa as ipa

def is_vow(character):
    '''
    Is the given (lowercase) character a vowel or not.
    '''
    #ipa_vowels = "iɪeɛæuʊoɔɑəʌ"
    ipa_vowels = "yøœɶɒɔoʊuʉiɪeɛæaɐɑʌɤɯɨɜ"
    return character in ipa_vowels

def is_space(character):
    '''
    Is the given character a space or newline (other space characters are
    cleaned in the preprocessing phase).
    '''
    return character==' ' or character=='\n'

def get_phonetic_transcription(lyrics):
    lines = lyrics.splitlines()
    phonetic_lines = [ipa.convert(line) for line in lines]
    phonetic = "\n".join(phonetic_lines)

    return phonetic

import re
import numpy as np



def syllable_similarity(bar1, bar2):
    similarity = abs(syllables.estimate(bar1) - syllables.estimate(bar2))
    return similarity

def get_syllable_count_difference(lyrics):
    lines = lyrics.splitlines()
    i = 0
    similarity = 0
    while i < len(lines)-1:
        similarity += syllable_similarity(lines[i], lines[i+1])
        i += 2
    return similarity/len(lines)

def get_rhyme_density(lyrics, lookback=15):
    bars = Lyrics(lyrics, lookback)
    return bars.avg_rhyme_length

def get_longest_rhyme(lyrics, lookback=15):
    bars = Lyrics(lyrics, lookback)
    return bars.get_longest_rhyme()[0]

def get_unique_words(lyrics):
    words = lyrics.split()
    unique_words = set(words)
    return len(unique_words)/len(words)

def print_lyrics_stats(lyrics, lookback=15, artist=None, title=None):
    bars = Lyrics(lyrics, lookback, artist, title)
    print(bars.title)
    print('------------------------------------------')
    print("Average rhyme length: %.3f\n" % bars.avg_rhyme_length)
    bars.print_rhyme(bars.longest_rhyme)
    print("Average syllable count difference: %.3f\n" % get_syllable_count_difference(lyrics))
    print("Percentage unique words: %.3f\n" % get_unique_words(lyrics))




class Lyrics:
    '''
    This class is used to store and preprocess rap lyrics and calculate
    statistics like average rhyme length out of the lyrics.
    '''

    def __init__(self, text=None, lookback=15, artist=None, title=None):
        '''
        Lyrics can be read from the file (default) or passed directly
        to this constructor.
        '''
        self.text_raw = None
        # How many previous words are checked for a rhyme.
        self.lookback = lookback
        self.text_raw = text
        if artist == None or title == None:
            self.title = "Generated Song"
        else:
            self.title = title + " by " + artist

        if self.text_raw is not None:
            cleaning_ok = self.clean_text(self.text_raw)
            self.compute_vowel_representation()
            self.avg_rhyme_length, self.longest_rhyme = self.rhyme_stats()

    def clean_text(self, text):
        '''
        Preprocess text by removing unwanted characters and duplicate rows.
        '''

        self.text = text
        # If there are more than 2 consecutive newlines, remove some of them
        # (just to make the cleaned text look prettier)
        self.text = re.sub('\n\n+', '\n\n', self.text)
        # Remove duplicate rows
        self.lines = self.text.split('\n')

        uniq_lines = set()
        new_text = ''
        for l in self.lines:
            l = l.strip()
            if len(l) > 0 and l in uniq_lines:
                continue
            # Remove lines that are within brackets/parenthesis
            if len(l) >= 2 and ((l[0]=='[' and l[-1]==']') or (l[0]=='(' and l[-1]==')')):
                continue
            uniq_lines.add(l)
            new_text += l + '\n'

        self.text = new_text

    def compute_vowel_representation(self):
        '''
        Compute a representation of the lyrics where only vowels are preserved.
        '''
        self.vow = [] # Lyrics with all but vowels removed
        self.vow_idxs = [] # Indices of the vowels in self.text list
        self.word_ends = [] # Indices of the last characters of each word
        self.words = [] # List of words in the lyrics
        self.line_idxs = []

        self.text_orig = self.text
        self.text = get_phonetic_transcription(self.text)
        self.word_ends_orig = []
        self.words_orig = []

        prev_space_idx = -1 # Index of the previous space char
        line_idx = 0 # Line index of the current character
        # Go through the lyrics char by char
        for i in range(len(self.text)):
            self.line_idxs.append(line_idx)
            c = self.text[i]
            #c = ph.map_vow(c)
            if is_vow(c):
                # Ignore double vowels
                # (in English this applies probably only to 'aa' as in 'bath'
                # which rhymes with 'trap' that has only 'a')
                if i > 0 and self.text[i-1] == c:
                    # Index of a double vowel points to the latter occurrence
                    self.vow_idxs[-1] = i
                    continue
                # TODO Diftongs should not be split (i.e. "price" should
                # not rhyme with "trap kit"). This has been fixed in BattleBot
                self.vow.append(c)
                self.vow_idxs.append(i)
            elif is_space(c):
                if c in '\n':
                    line_idx += 1
                elif c in '.!?' and i < len(self.text)-1 and self.text[i+1] != '\n':
                    line_idx += 1
                # If previous char was not a space, we've encountered word end
                if len(self.vow) > 0 and not is_space(self.text[i-1]):
                    # Put together the new word. Potential consonants in the
                    # end are ignored
                    new_word = self.text[prev_space_idx+1:self.vow_idxs[-1]+1]
                    # Check that the new word contains at least one vowel
                    no_vowels = True
                    for c2 in new_word:
                        if is_vow(c2):
                            no_vowels = False
                            break
                    if no_vowels:
                        prev_space_idx = i
                        continue
                    self.word_ends.append(len(self.vow)-1)
                    self.words.append(new_word)
                prev_space_idx = i

        self.lines_orig = self.text_orig.split('\n')

    def rhyme_length(self, wpos2):
        '''
        Length of rhyme (in vowels). The latter part of the rhyme ends with
        word self.words[wpos2].

        Input:
            wpos2       Word index of the end of the rhyme.
        '''
        max_length = 0
        max_wpos1 = None
        wpos1 = max(0,wpos2-self.lookback)
        while wpos1 < wpos2:
            rl = self.rhyme_length_fixed(wpos1, wpos2)
            if rl > max_length:
                max_length = rl
                max_wpos1 = wpos1
            wpos1 += 1
        return max_length, max_wpos1

    def rhyme_length_fixed(self, wpos1, wpos2):
        '''
        Length of rhyme (in vowels). The first part of the rhyme ends with
        self.words[wpos1] and the latter part with word self.words[wpos2].

        Input:
            wpos1       Word index of the last word in the first part of the rhyme.
            wpos2       Word index of the end of the rhyme.
        '''
        if wpos1 < 0: # Don't wrap
            return 0
        elif self.words[wpos1] == self.words[wpos2]:
            return 0
        # Indices in the vowel list
        p1 = self.word_ends[wpos1]
        p2 = self.word_ends[wpos2]
        l = 0
        while self.vow[p1-l] == self.vow[p2-l]:
            # Make sure that exactly same words are not used
            if wpos1 > 0 and p1-l <= self.word_ends[wpos1-1] and wpos2 > 0 and p2-l <= self.word_ends[wpos2-1]:
                # Get the first and last character indices of the words surrounding the vowels at p1-l and p2-l
                prev_s1 = self.vow_idxs[p1-l]
                while prev_s1 > 0 and not is_space(self.text[prev_s1-1]):
                    prev_s1 -= 1
                prev_s2 = self.vow_idxs[p2-l]
                while prev_s2 > 0 and not is_space(self.text[prev_s2-1]):
                    prev_s2 -= 1
                next_s1 = self.vow_idxs[p1-l]
                while next_s1 < len(self.text)-1 and not is_space(self.text[next_s1+1]):
                    next_s1 += 1
                next_s2 = self.vow_idxs[p2-l]
                while next_s2 < len(self.text)-1 and not is_space(self.text[next_s2+1]):
                    next_s2 += 1
                if next_s1-prev_s1 == next_s2-prev_s2 and self.text[prev_s1:next_s1+1] ==  self.text[prev_s2:next_s2+1]:
                    break

            l += 1
            if p1-l < 0 or p2-l <= p1:
                break
        # Ignore rhymes with length 1
        if l == 1:
            l = 0
        return l

    def rhyme_stats(self):
        '''
        Compute the average rhyme length of the song and the longest rhyme.

        Output:
            Average rhyme length (float)
            Longest rhyme which is a 3-tuple with:
                (length, word index of the first part of the rhyme,
                         word index of the latter part of the rhyme)
        '''
        # Rhyme length of each word
        rls = []
        # Keep track of the longest rhyme
        max_rhyme = (0,None,None)
        for wpos2 in range(1,len(self.word_ends)):
            (rl, wpos1) = self.rhyme_length(wpos2)
            rls.append(rl)
            if rl > max_rhyme[0]:
                max_rhyme = (rl, wpos1, wpos2)
        rls = np.array(rls)
        # Average rhyme length of the song
        if len(rls) > 0:
            avg_rl = np.mean(rls)
        else:
            avg_rl = 0
        return avg_rl, max_rhyme

    def get_avg_rhyme_length(self):
        return self.avg_rhyme_length

    def print_song_stats(self):
        print('------------------------------------------')
        print("Avg rhyme length: %.3f\n" % self.avg_rhyme_length)

        self.print_rhyme(self.longest_rhyme)


    def print_rhyme(self, rhyme_tuple):
        print(self.get_rhyme_str(rhyme_tuple))

    def get_rhyme_str(self, rhyme_tuple):
        '''
        Construct a string of a given rhyme tuple.
        '''
        ret = ''
        rl, wpos1, wpos2 = rhyme_tuple
        if wpos1 is None or wpos2 is None:
            return ''
        p2 = self.vow_idxs[self.word_ends[wpos2]]
        p2_orig = p2
        # Find the ending of the last word
        while not is_space(self.text[p2]):
            p2 += 1
        p0 = self.vow_idxs[self.word_ends[wpos1]-rl]
        p0_orig = p0
        # Find the beginning of the line
        while self.text[p0] != '\n' and p0 > 0:
            p0 -= 1

        cap_line = ''
        rw1, rw2 = self.get_rhyming_vowels(rhyme_tuple)
        for i in range(p0,p2+1):
            if i == min(rw1) or i == min(rw2):
                cap_line += ' | ' + self.text[i]
            elif i == max(rw1) or i == max(rw2):
                cap_line += self.text[i] + '|'
            else:
                cap_line += self.text[i]
        ret += "Longest rhyme (l=%d): %s\n" % (rl, cap_line)
        # Get the corresponding lines from the original lyrics
        line_beg = self.line_idxs[p0]
        line_end = self.line_idxs[p2]
        for i in range(line_beg, line_end+1):
            if i < len(self.lines_orig):
                ret += self.lines_orig[i] + '\n'
        return ret

    def get_longest_rhyme(self):
        rhyme_str = self.get_rhyme_str(self.longest_rhyme)
        return self.longest_rhyme[0], rhyme_str

    def get_rhyming_vowels(self, rhyme_tuple):
        '''
        Return the indices of the rhyming vowels of the longest rhyme.

        Output:
            Tuple with the indices of the first part and the second part of
            the rhyme separately.
        '''
        rl, wpos1, wpos2 = rhyme_tuple
        if wpos1 is None or wpos2 is None:
            return ([-1],[-1])

        # The first part of the rhyme
        rhyme_idxs1 = [] # Indices of the rhyming vowels
        n_caps = 0
        p = self.vow_idxs[self.word_ends[wpos1]]
        while n_caps < rl:
            if is_vow(self.text[p]):
                rhyme_idxs1.append(p)
                # Increase the counter only if the vowel is not a double vowel
                if self.text[p] != self.text[p+1]:
                    n_caps += 1
            p -= 1

        # The second part of the rhyme
        rhyme_idxs2 = [] # Indices of the rhyming vowels
        n_caps = 0
        p = self.vow_idxs[self.word_ends[wpos2]]
        p_last = p
        while n_caps < rl:
            if is_vow(self.text[p]):
                rhyme_idxs2.append(p)
                # Increase the counter only if the vowel is not a double vowel.
                # The last vowel must be always counted.
                if p == p_last or self.text[p] != self.text[p+1]:
                    n_caps += 1
            p -= 1

        return (rhyme_idxs1, rhyme_idxs2)

import numpy as np
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.util import bigrams, ngrams, everygrams
from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends
from nltk.lm import MLE, KneserNeyInterpolated, Lidstone, Laplace, AbsoluteDiscountingInterpolated

nltk.download('punkt')

final_verse = "Thick if your h-aven girl problems I feel bad fa yeah, yeah (yeah, yeah, yeah, yeah, yeah, yeah) four, four, four, four, fou Thicker than awake like yourself If I wa Lips blow at steak for your ni-gaes with your tra-ping Big-lation problem yeah I know, you just got cash (mm, bust it) just awaken shaken once again, ho, you know it's on) Bye bye when we going s-layed"
result_final = re.sub(r'[ ]+\n[ ]+', r'\n', ''.join(final_verse))
print(result_final)

def get_rhyme_density(lyrics, lookback=15):
    bars = Lyrics(lyrics, lookback)
    return bars.avg_rhyme_length
results_rd = np.array(get_rhyme_density(result_final))

round(np.mean(results_rd),2)

results_lr = np.array(get_longest_rhyme(result_final))

round(np.mean(results_lr),2)

results_uw = np.array(get_unique_words(result_final))

print(round(np.mean(results_uw),2))