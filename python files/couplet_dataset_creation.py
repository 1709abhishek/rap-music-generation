# -*- coding: utf-8 -*-
"""couplet_dataset_creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hc4V1wh4MQncglJMXeAdrUTkIdoVE052
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from google.colab import drive
drive.mount('/content/MyDrive')

import pandas as pd

df = pd.read_csv("/content/MyDrive/MyDrive/NLP Project/lyrics.csv")
print(df.head())
df = df[["Title","Artist", "Lyrics"]]
df.rename({"Lyrics": "lyrics_grapheme", "Title": "title", "Artist": "artist"}, axis=1, inplace=True)
print(df.head())
df.head()
df.describe()

"""## Clean Lyrics"""

#apply regex to remove "Embed" from end of each lyric
import re
pattern = r"\d+Embed$"

df["lyrics_grapheme"] = df["lyrics_grapheme"].str.replace(pattern, "")

#convert to lowercase
df["lyrics_grapheme"] = df["lyrics_grapheme"].apply(lambda x: x.lower())

#drop duplicates
df.drop_duplicates(inplace=True)

#drop languages that aren't english
!pip install langdetect
from langdetect import detect

def detect_lyric(x):
    try:
        return detect(x)
    except:
        return

df["language"] = df["lyrics_grapheme"].apply(detect_lyric)
df= df[df.language == "en"]

#drop all empty lyrics
df = df[df["lyrics_grapheme"]!=""]

lyrics = df["lyrics_grapheme"].to_list()
print(lyrics[0])

lyrics = list(map(lambda x: x.replace("::", ":"),lyrics))
print(lyrics[0])

verses_grapheme = [lyric.split(":") for lyric in lyrics]

import itertools

verses_grapheme = list(itertools.chain(*verses_grapheme))
display(verses_grapheme)

def couplets_create(verses: list):
    couplets = []
    for i in range(1,len(verses)):
        couplet = verses[i-1] + "\n" + verses[i]
        couplets.append(couplet)

    return couplets


final_couplets = couplets_create(verses_grapheme)
#display(couplets)

df_couplets_and_verses = pd.DataFrame(final_couplets, columns=["couplets_g"])

! pip install phyme

"""https://github.com/jameswenzel/Phyme#"""

import re

def get_last_words(couplet):
    last_words = []
    lines = couplet.split("\n")
    for line in lines:
        line_words = line.split(" ")
        last_word = re.sub(r"[^a-zA-Z]+", "",line_words[-1]) #remove everything that is not a letter from last word
        last_words.append(last_word)
    return last_words


df_couplets_and_verses["last_words"] = df_couplets_and_verses["couplets_g"].apply(get_last_words)

import itertools
import re
from Phyme import Phyme

phy = Phyme()


def check_rhyming_scheme(words: list, phy: Phyme) -> bool:
    if len(words[0])==1:
        return False
    try:
        # get rhymes for first word in list and reformat output dictionary into a list
        rhyming_words = ph.get_perfect_rhymes(words[0]).values()
        rhyming_words = list(itertools.chain(*rhyming_words))
        pattern = "\(\d\)" # remove (2)
        rhyming_words = [re.sub(pattern,"", rhyme) for rhyme in rhyming_words]
        if words[1] in rhyming_words:
            return True
        else:
            return False
    except KeyError as ke:
        return f"{ke} NOT FOUND"

df_couplets_and_verses["rhyme"] = df_couplets_and_verses.apply(lambda row: check_rhyming_scheme(row["last_words"],ph),axis=1)

display(df_couplets_and_verses)
df_couplets_and_verses.groupby("rhyme").count()

df_rhyming_couplets_and_words = df_couplets_and_verses[df_couplets_and_verses["rhyme"] ==True]
display(df_rhyming_couplets_and_words)

def split_couplet(couplet):
    lines = couplet.split("\n")
    return lines


df_rhyming_couplets_and_words["couplet_split"] = df_rhyming_couplets_and_words["couplets_g"].apply(split_couplet)
df_rhyming_couplets_and_words["line1_g"] = df_rhyming_couplets_and_words["couplet_split"].apply(lambda x: x[0])
df_rhyming_couplets_and_words["line2_g"] = df_rhyming_couplets_and_words["couplet_split"].apply(lambda x: x[1])
display(df_rhyming_couplets_and_words)

#remove boring couplets where line 1 is same as line 2
df_rhyming_couplets_and_words = df_rhyming_couplets_and_words[df_rhyming_couplets_and_words["line1_g"]!= df_rhyming_couplets_and_words["line2_g"]]
display(df_rhyming_couplets_and_words)

# write to csv so it can be phonemized
df_rhyming_couplets_and_words.to_csv("/content/MyDrive/MyDrive/NLP Project/rhyme_couplets.csv",index=False)

!pip install phonemizer
from phonemizer import phonemize, version
from phonemizer.separator import Separator
import pandas as pd
from datetime import datetime

!sudo apt-get install festival espeak-ng mbrola

# from festival import festival
input_file = "/content/MyDrive/MyDrive/NLP Project/rhyme_couplets.csv"
df_couplets_and_verses = pd.read_csv(input_file)

line1_g =df_couplets_and_verses["line1_g"]
line2_g =df_couplets_and_verses["line2_g"]
# print(line1_g)

line1_p = []
line2_p = []
b_size =200

for i in range(0,len(df_couplets_and_verses),b_size):
#prepare batches for efficiency while phoonemizing
  if i + b_size<len(df_couplets_and_verses):
    first_batch = line1_g[i:i+b_size]
    second_batch = line2_g[i:i+b_size]
  else:
    first_batch = line1_g[i:len(df_couplets_and_verses)]
    second_batch = line2_g[i:len(df_couplets_and_verses)]
  # try:
    print(first_batch)

    first_batch_p = phonemize(first_batch, language='en-us',
    backend='festival',separator=Separator(phone="-", word=' ',
    syllable='|'), strip=True)
    print(first_batch_p)

    second_batch_p = phonemize(second_batch, language='en-us',
    backend='festival',separator=Separator(phone="-", word=' ',
    syllable='|'), strip=True)

    line1_p.extend(first_batch_p)
    line2_p.extend(second_batch_p)

df_couplets_and_verses["line1_p"] = line1_p
df_couplets_and_verses["line2_p"] = line2_p

df_couplets_and_verses.to_csv("/content/MyDrive/MyDrive/NLP Project/rhyme_couplets_f-phonemized_07-30-23.csv", index=False)

"""# ------------Reload Phonemized CSV and Assemble tasks for training----------"""

import pandas as pd
couplets_group_df = pd.read_csv("/content/MyDrive/MyDrive/NLP Project/rhyme_couplets_f-phonemized_07-30-23.csv")

display(couplets_group_df)

#check for lines rthat failed to phonemize
couplets_group_df[(couplets_group_df["line1_p"] == None) |(couplets_group_df["line2_p"] == None)]

import random

# create tasks for multi-task learning
# < line 1 grapheme =1G|2G= line 2 grapheme>
# <line 1 phoneme =1P|2P= line 2 phoneme>
# [ line 1 grapheme =1G|1P= line 1 phoneme]
# [line 1 phoneme =1P|1G= line 1 grapheme ]
# [line 2 grapheme =2G|2P=line 2 phoneme]
# [line 2 phoneme =2P|2G= line 2 grapheme]


line1_grapheme = couplets_group_df["line1_g"].to_list()
line2_grapheme = couplets_group_df["line2_g"].to_list()
line1_phoneme = couplets_group_df["line1_p"].to_list()
line2_phoneme = couplets_group_df["line2_p"].to_list()

pnc = []

for i in range(len(line1_grapheme)):
    pnc.append(f"~ {line1_grapheme[i]} =1G->2G= {line2_grapheme[i]} ~")
    pnc.append(f"~ {line1_phoneme[i]} =1P->2P= {line2_phoneme[i]} ~")
    pnc.append(f"[ {line1_grapheme[i]} =1G->1P= {line1_phoneme[i]} ]")
    pnc.append(f"[ {line1_phoneme[i]} =1P->1G= {line1_grapheme[i]} ]")
    pnc.append(f"[ {line2_grapheme[i]} =2G->2P= {line2_phoneme[i]} ]")
    pnc.append(f"[ {line2_phoneme[i]} =2P->2G= {line2_grapheme[i]} ]")

random.shuffle(pnc)

display(len(pnc))
train_test_split = round(0.99*len(pnc))

train_data = pnc[:train_test_split]
test_data = pnc[train_test_split:]

with open("/content/MyDrive/MyDrive/NLP Project/train_couplets.txt", "w") as f:
    for task in train_data:
        f.write(task+"\n")

with open("/content/MyDrive/MyDrive/NLP Project/test_couplets.txt", "w") as f:
    for task in test_data:
        f.write(task+"\n")



