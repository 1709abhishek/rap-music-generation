# -*- coding: utf-8 -*-
"""couplet_dataset_creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hc4V1wh4MQncglJMXeAdrUTkIdoVE052

## Summary
The notebook's primary function is to transform a dataset of song lyrics from simple text to a structured multi-task learning dataset, suitable for teaching a model both phoneme recognition and couplet generation. Starting with a .csv file, it cleans the data and then transforms the song lyrics into discrete lines. These lines are then paired into rhyming couplets. The culmination of the notebook's process is the creation of a dataset that serves dual purposes: it facilitates phoneme-to-grapheme translation (and vice versa) and bridges the first line to the second in couplets. This dual-purpose dataset is instrumental for the model to not only anticipate the subsequent line in a couplet but also to comprehend the rhythm and meter inherent in the song lyrics, attributed to the phonemic patterns.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from google.colab import drive
drive.mount('/content/MyDrive')

import pandas as pd

lyrics_df = pd.read_csv("/content/MyDrive/MyDrive/NLP Project/lyrics.csv")
print(lyrics_df.head())
lyrics_df = lyrics_df[["Title","Artist", "Lyrics"]]
lyrics_df.rename({"Lyrics": "lyrics_g", "Title": "title", "Artist": "artist"}, axis=1, inplace=True)
print(lyrics_df.head())
lyrics_df.head()
lyrics_df.describe()

"""## Clean Lyrics"""

#apply regex to remove "Embed" from end of each lyric
import re
pattern = r"\d+Embed$"

lyrics_df["lyrics_g"] = lyrics_df["lyrics_g"].str.replace(pattern, "")

#convert to lowercase
lyrics_df["lyrics_g"] = lyrics_df["lyrics_g"].apply(lambda x: x.lower())

#drop duplicates
lyrics_df.drop_duplicates(inplace=True)

#drop languages that aren't english
!pip install langdetect
from langdetect import detect

def detect_lyric(x):
    try:
        return detect(x)
    except:
        return

lyrics_df["language"] = lyrics_df["lyrics_g"].apply(detect_lyric)
lyrics_df= lyrics_df[lyrics_df.language == "en"]

lyrics_df = lyrics_df[lyrics_df["artist"] != "Anuel AA"]

#drop all empty lyrics
lyrics_df = lyrics_df[lyrics_df["lyrics_g"]!=""]

lyrics = lyrics_df["lyrics_g"].to_list()
print(lyrics[0])

lyrics = list(map(lambda x: x.replace("::", ":"),lyrics))
print(lyrics[0])

verses = [lyric.split(":") for lyric in lyrics]

import itertools

verses = list(itertools.chain(*verses))
display(verses)

def create_couplets(verses: list):
    couplets = []
    for i in range(1,len(verses)):
        couplet = verses[i-1] + "\n" + verses[i]
        couplets.append(couplet)

    return couplets


couplets = create_couplets(verses)
#display(couplets)

couplets_df = pd.DataFrame(couplets, columns=["couplets_g"])

! pip install phyme

! /Users/austinpaxton/anaconda3/envs/lyric_generation_capstone/bin/python3.10 -m pip install phyme

"""https://github.com/jameswenzel/Phyme#"""

import re

def get_last_words(couplet):
    last_words = []
    lines = couplet.split("\n")
    for line in lines:
        line_words = line.split(" ")
        last_word = re.sub(r"[^a-zA-Z]+", "",line_words[-1]) #remove everything that is not a letter from last word
        last_words.append(last_word)
    return last_words


couplets_df["last_words"] = couplets_df["couplets_g"].apply(get_last_words)

import itertools
import re
from Phyme import Phyme

ph = Phyme()


def check_perfect_rhyme(words: list, ph: Phyme) -> bool:
    if len(words[0])==1:
        return False
    try:
        # get rhymes for first word in list and reformat output dictionary into a list
        rhymes = ph.get_perfect_rhymes(words[0]).values()
        rhymes = list(itertools.chain(*rhymes))
        pattern = "\(\d\)" # remove (2)
        rhymes = [re.sub(pattern,"", rhyme) for rhyme in rhymes]
        if words[1] in rhymes:
            return True
        else:
            return False
    except KeyError as ke:
        return f"{ke} NOT FOUND"

couplets_df["rhyme"] = couplets_df.apply(lambda row: check_perfect_rhyme(row["last_words"],ph),axis=1)

display(couplets_df)
couplets_df.groupby("rhyme").count()

rhyme_couplets_df = couplets_df[couplets_df["rhyme"] ==True]
display(rhyme_couplets_df)

def split_couplet(couplet):
    lines = couplet.split("\n")
    return lines


rhyme_couplets_df["couplet_split"] = rhyme_couplets_df["couplets_g"].apply(split_couplet)
rhyme_couplets_df["line1_g"] = rhyme_couplets_df["couplet_split"].apply(lambda x: x[0])
rhyme_couplets_df["line2_g"] = rhyme_couplets_df["couplet_split"].apply(lambda x: x[1])
display(rhyme_couplets_df)

#remove boring couplets where line 1 is same as line 2
rhyme_couplets_df = rhyme_couplets_df[rhyme_couplets_df["line1_g"]!= rhyme_couplets_df["line2_g"]]
display(rhyme_couplets_df)

# write to csv so it can be phonemized
rhyme_couplets_df.to_csv("/content/MyDrive/MyDrive/NLP Project/rhyme_couplets.csv",index=False)

!pip install phonemizer
from phonemizer import phonemize, version
from phonemizer.separator import Separator
import pandas as pd
from datetime import datetime

!sudo apt-get install festival espeak-ng mbrola

# from festival import festival
input_file = "/content/MyDrive/MyDrive/NLP Project/rhyme_couplets.csv"
couplets_df = pd.read_csv(input_file)

line1_g =couplets_df["line1_g"]
line2_g =couplets_df["line2_g"]
# print(line1_g)

line1_p = []
line2_p = []
batch_size =200

for i in range(0,len(couplets_df),batch_size):
#prepare batches for efficiency while phoonemizing
  if i + batch_size<len(couplets_df):
    batch1 = line1_g[i:i+batch_size]
    batch2 = line2_g[i:i+batch_size]
  else:
    batch1 = line1_g[i:len(couplets_df)]
    batch2 = line2_g[i:len(couplets_df)]
  # try:
    print(batch1)
    #phonemize batches
    batch1_p = phonemize(batch1, language='en-us',
    backend='festival',separator=Separator(phone="-", word=' ',
    syllable='|'), strip=True)
    print(batch1_p)

    batch2_p = phonemize(batch2, language='en-us',
    backend='festival',separator=Separator(phone="-", word=' ',
    syllable='|'), strip=True)

    line1_p.extend(batch1_p)
    line2_p.extend(batch2_p)

  # except:
  #   print(f"phonemization failed for batch:{i}")
  #   # Add None placeholders for failed batches
  #   line1_p.extend([None] * len(batch1))
  #   line2_p.extend([None] * len(batch2))

couplets_df["line1_p"] = line1_p
couplets_df["line2_p"] = line2_p
# phonemize_df(couplets_df, "line1_g", "line1_p")
# phonemize_df(couplets_df, "line2_g", "line2_p")

couplets_df.to_csv("/content/MyDrive/MyDrive/NLP Project/rhyme_couplets_f-phonemized_07-30-23.csv", index=False)

"""# ------------Reload Phonemized CSV and Assemble tasks for training----------"""

import pandas as pd
couplets_gp_df = pd.read_csv("/content/MyDrive/MyDrive/NLP Project/rhyme_couplets_f-phonemized_07-30-23.csv")

display(couplets_gp_df)

#check for lines rthat failed to phonemize
couplets_gp_df[(couplets_gp_df["line1_p"] == None) |(couplets_gp_df["line2_p"] == None)]

import random

# create tasks for multi-task learning
# < line 1 grapheme =1G|2G= line 2 grapheme>
# <line 1 phoneme =1P|2P= line 2 phoneme>
# [ line 1 grapheme =1G|1P= line 1 phoneme]
# [line 1 phoneme =1P|1G= line 1 grapheme ]
# [line 2 grapheme =2G|2P=line 2 phoneme]
# [line 2 phoneme =2P|2G= line 2 grapheme]


line1_g = couplets_gp_df["line1_g"].to_list()
line2_g = couplets_gp_df["line2_g"].to_list()
line1_p = couplets_gp_df["line1_p"].to_list()
line2_p = couplets_gp_df["line2_p"].to_list()

tasks = []

for i in range(len(line1_g)):
    tasks.append(f"~ {line1_g[i]} =1G->2G= {line2_g[i]} ~")
    tasks.append(f"~ {line1_p[i]} =1P->2P= {line2_p[i]} ~")
    tasks.append(f"[ {line1_g[i]} =1G->1P= {line1_p[i]} ]")
    tasks.append(f"[ {line1_p[i]} =1P->1G= {line1_g[i]} ]")
    tasks.append(f"[ {line2_g[i]} =2G->2P= {line2_p[i]} ]")
    tasks.append(f"[ {line2_p[i]} =2P->2G= {line2_g[i]} ]")

random.shuffle(tasks)

display(len(tasks))
train_test_split = round(0.99*len(tasks))

couplets_train = tasks[:train_test_split]
couplets_test = tasks[train_test_split:]

with open("/content/MyDrive/MyDrive/NLP Project/train_couplets.txt", "w") as f:
    for task in couplets_train:
        f.write(task+"\n")

with open("/content/MyDrive/MyDrive/NLP Project/test_couplets.txt", "w") as f:
    for task in couplets_test:
        f.write(task+"\n")



